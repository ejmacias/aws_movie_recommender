{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading files\n",
    "ratings = pd.read_csv('../data/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users with the least amount of ratings?\n",
    "ratingsByUser = ratings.groupby('userId', as_index=False)['movieId'].count()\n",
    "ratingsByUser.rename(columns = {'movieId':'numRatings'}, inplace = True)\n",
    "ratingsByUser.loc[ratingsByUser['numRatings'] == ratingsByUser['numRatings'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users with the higuest number of ratings?\n",
    "ratingsByUser.loc[ratingsByUser['numRatings'] == ratingsByUser['numRatings'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movies with the least amount of ratings?\n",
    "ratingsByMovie = ratings.groupby('movieId', as_index=False)['userId'].count()\n",
    "ratingsByMovie.rename(columns = {'userId':'numRatings'}, inplace = True)\n",
    "ratingsByMovie.loc[ratingsByMovie['numRatings'] == ratingsByMovie['numRatings'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movies with the higuest number of ratings?\n",
    "type(ratingsByMovie.loc[ratingsByMovie['numRatings'] == ratingsByMovie['numRatings'].max()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie title converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('../data/movies.csv', usecols=[0,1], index_col=0, squeeze=True).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surprise SciKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../data/ratings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ratings_path = '../data/ratings.csv'\n",
    "\n",
    "reader = Reader(line_format='user item rating timestamp', sep=',', rating_scale = (0.5,5.0), skip_lines=1)\n",
    "\n",
    "data = Dataset.load_from_file(ratings_path, reader=reader)\n",
    "\n",
    "#trainset, testset = train_test_split(data, test_size=0.2)\n",
    "trainset = data.build_full_trainset() # cross-validation will be applied for the evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD_model = SVD(lr_all=0.005, reg_all=0.02)\n",
    "SVD_model.fit(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results = cross_validate(\n",
    "    algo = SVD_model, data = data, measures=['RMSE'], \n",
    "    cv=5, return_train_measures=True\n",
    "    )\n",
    "results['test_rmse'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD_model.predict(uid='1', iid='6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "movies.loc[movies['movieId'] == 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ratings.loc[(ratings['userId'] == 1) & (ratings['rating'] == 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "movies.loc[movies['movieId'].isin([47,50])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_factors': [5], #[10, 100, 500],\n",
    "    'n_epochs': [5], #[5, 20, 50], \n",
    "    'lr_all': [0.005], #[0.001, 0.005, 0.02],\n",
    "    'reg_all': [0.02]} #[0.005, 0.02, 0.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_model = GridSearchCV(\n",
    "    algo_class = SVD,\n",
    "    param_grid = param_grid,\n",
    "    n_jobs = -1,\n",
    "    joblib_verbose = 25,\n",
    "    return_train_measures=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_model.fit(data)\n",
    "\n",
    "gs_model.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_SVD = gs_model.best_estimator['rmse']\n",
    "best_SVD.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_model.cv_results['mean_test_rmse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test RMSE = {gs_model.cv_results['mean_test_rmse'][0].round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_USER = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "ratings = pd.read_csv('../data/ratings.csv')\n",
    "movies = pd.read_csv('../data/movies.csv')\n",
    "unrated_movie_ids = [movieId for movieId in movies['movieId'] if not movieId in ratings.loc[(ratings['userId'] == TEST_USER), 'movieId'].tolist()]\n",
    "unrated_movie_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predictions = [()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best_SVD.predict(uid = str(TEST_USER), iid = '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    \"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        print(f'iid: {iid} - title:{movies[int(iid)]}')\n",
    "        top_n[uid].append((movies[int(iid)], est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Then predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "testset = trainset.build_anti_testset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#It's important to reduce the testset for the given user only, otherwise it takes too long to predict\n",
    "\n",
    "testset = [user for user in testset if user[0] == TEST_USER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "full_predictions = best_SVD.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(full_predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_predictions[0].uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# To Do: full predictions will be loaded, I need to filter the ones for the given user only\n",
    "#############################\n",
    "user_predictions = [prediction for prediction in full_predictions if prediction.uid == TEST_USER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(user_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "top_ratings = get_top_n(user_predictions, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_movies = [movie[0] for movie in top_ratings[TEST_USER]]\n",
    "recommended_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump.dump('modelo', algo=best_SVD, predictions=full_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_predictions, mi_modelo = dump.load('modelo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_predictions = [prediction for prediction in full_predictions if prediction.uid == TEST_USER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(full_predictions))\n",
    "print(len(user_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
